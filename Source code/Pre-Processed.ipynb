{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95bf6a0c-eda9-4b36-821e-7f3e115e6d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Unnamed: 0: integer (nullable = true)\n",
      " |-- trans_date_trans_time: timestamp (nullable = true)\n",
      " |-- cc_num: long (nullable = true)\n",
      " |-- merchant: string (nullable = true)\n",
      " |-- category: string (nullable = true)\n",
      " |-- amt: double (nullable = true)\n",
      " |-- first: string (nullable = true)\n",
      " |-- last: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- street: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- zip: integer (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- city_pop: integer (nullable = true)\n",
      " |-- job: string (nullable = true)\n",
      " |-- dob: timestamp (nullable = true)\n",
      " |-- trans_num: string (nullable = true)\n",
      " |-- unix_time: integer (nullable = true)\n",
      " |-- merch_lat: double (nullable = true)\n",
      " |-- merch_long: double (nullable = true)\n",
      " |-- is_fraud: integer (nullable = true)\n",
      " |-- merch_zipcode: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Credit Card Fraud Detection\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load CSV correctly (no line break!)\n",
    "df = spark.read.csv(\n",
    "    \"hdfs:///user/talentum/NFeatures/credit_card_transactions.csv\",\n",
    "    header=True,\n",
    "    inferSchema=True\n",
    ")\n",
    "\n",
    "# Show schema\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8bd8a8f-1f65-46f7-bdac-d5f86ee50b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1296675, 24)\n"
     ]
    }
   ],
   "source": [
    "rows = df.count()\n",
    "cols = len(df.columns)\n",
    "print(f\"Shape: ({rows}, {cols})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "981fd269-0477-42d2-9b4e-b804ddd2a029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------+------+--------+--------+---+-----+----+------+------+----+-----+---+---+----+--------+---+---+---------+---------+---------+----------+--------+-------------+\n",
      "|Unnamed: 0|trans_date_trans_time|cc_num|merchant|category|amt|first|last|gender|street|city|state|zip|lat|long|city_pop|job|dob|trans_num|unix_time|merch_lat|merch_long|is_fraud|merch_zipcode|\n",
      "+----------+---------------------+------+--------+--------+---+-----+----+------+------+----+-----+---+---+----+--------+---+---+---------+---------+---------+----------+--------+-------------+\n",
      "|         0|                    0|     0|       0|       0|  0|    0|   0|     0|     0|   0|    0|  0|  0|   0|       0|  0|  0|        0|        0|        0|         0|       0|       195973|\n",
      "+----------+---------------------+------+--------+--------+---+-----+----+------+------+----+-----+---+---+----+--------+---+---+---------+---------+---------+----------+--------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum, when\n",
    "\n",
    "null_counts = df.select([sum(when(col(c).isNull(), 1).otherwise(0)).alias(c) for c in df.columns])\n",
    "null_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b6e93a8-1636-4b58-9273-e7026c6d0ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('merch_zipcode')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f01f1c8b-c649-43f4-b0d2-38718b57af7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0',\n",
       " 'trans_date_trans_time',\n",
       " 'cc_num',\n",
       " 'merchant',\n",
       " 'category',\n",
       " 'amt',\n",
       " 'first',\n",
       " 'last',\n",
       " 'gender',\n",
       " 'street',\n",
       " 'city',\n",
       " 'state',\n",
       " 'zip',\n",
       " 'lat',\n",
       " 'long',\n",
       " 'city_pop',\n",
       " 'job',\n",
       " 'dob',\n",
       " 'trans_num',\n",
       " 'unix_time',\n",
       " 'merch_lat',\n",
       " 'merch_long',\n",
       " 'is_fraud']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0c79780-8e56-484b-aeb1-ef397f1663a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, datediff, to_date, floor\n",
    "\n",
    "# Convert to date format (if not already)\n",
    "df = df.withColumn(\"dob\", to_date(col(\"dob\")))\n",
    "df = df.withColumn(\"trans_date_trans_time\", to_date(col(\"trans_date_trans_time\")))\n",
    "\n",
    "# Calculate age in years\n",
    "df = df.withColumn(\"age\", floor(datediff(col(\"trans_date_trans_time\"), col(\"dob\")) / 365))\n",
    "\n",
    "# Drop the 'dob' column\n",
    "df = df.drop(\"dob\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de2e139d-30e3-4021-9c49-f4e79b814599",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "# Define the Python function\n",
    "def calculate_distance(lat1, lon1, lat2, lon2):\n",
    "    try:\n",
    "        return geodesic((lat1, lon1), (lat2, lon2)).km\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "# Register as UDF\n",
    "distance_udf = udf(calculate_distance, DoubleType())\n",
    "\n",
    "# Apply the UDF to create the 'distance' column\n",
    "df = df.withColumn(\"distance\", distance_udf(\n",
    "    col(\"lat\"), col(\"long\"), col(\"merch_lat\"), col(\"merch_long\")\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e60bc975-2d4a-4c71-9a33-0746dad2e915",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "\n",
    "df = df.withColumn(\"amt_per_capita\", col(\"amt\") / (col(\"city_pop\") + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7b3ee09-a1e2-4bb1-8031-87020cb2ec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Get 95th percentile\n",
    "q95 = df.approxQuantile(\"distance\", [0.95], 0.01)[0]\n",
    "\n",
    "# Step 2: Create binary 'is_far' column\n",
    "from pyspark.sql.functions import col, when\n",
    "\n",
    "df = df.withColumn(\"is_far\", when(col(\"distance\") > q95, 1).otherwise(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9880a495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import hour, dayofmonth, dayofweek, col\n",
    "\n",
    "df = df.withColumn(\"trans_hour\", hour(col(\"trans_date_trans_time\"))) \\\n",
    "       .withColumn(\"trans_day\", dayofmonth(col(\"trans_date_trans_time\"))) \\\n",
    "       .withColumn(\"trans_weekday\", dayofweek(col(\"trans_date_trans_time\")) - 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c6f35e2-f7fd-4a6e-8b6a-13018226f200",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Step 1: Define the Python function\n",
    "def hour_bucket(hour):\n",
    "    if 0 <= hour < 6:\n",
    "        return 'night'\n",
    "    elif 6 <= hour < 12:\n",
    "        return 'morning'\n",
    "    elif 12 <= hour < 18:\n",
    "        return 'afternoon'\n",
    "    else:\n",
    "        return 'evening'\n",
    "\n",
    "# Step 2: Register UDF\n",
    "hour_bucket_udf = udf(hour_bucket, StringType())\n",
    "\n",
    "# Step 3: Apply UDF to create new column\n",
    "df = df.withColumn(\"hour_bucket\", hour_bucket_udf(col(\"trans_hour\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49b2346a-1c45-49a2-8c42-006d1d967572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Step 1: Create StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"hour_bucket\", outputCol=\"hour_bucket_encoded\")\n",
    "\n",
    "# Step 2: Fit and transform\n",
    "df = indexer.fit(df).transform(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b8bd11d4-5163-438e-9b51-3cd3e0789bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('hour_bucket')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82278c07-2461-461d-9fde-08cfd9e2ea57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "\n",
    "# STEP 1: StringIndexer + OneHotEncoder\n",
    "categorical_cols = ['category', 'gender']\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col + \"_index\") for col in categorical_cols]\n",
    "encoders = [OneHotEncoder(inputCol=col + \"_index\", outputCol=col + \"_vec\", dropLast=True) for col in categorical_cols]\n",
    "\n",
    "# STEP 2: Build and run pipeline\n",
    "pipeline = Pipeline(stages=indexers + encoders)\n",
    "model = pipeline.fit(df)\n",
    "df = model.transform(df)\n",
    "\n",
    "# STEP 3: Convert vector to array\n",
    "def vector_to_array(v):\n",
    "    return v.toArray().tolist()\n",
    "\n",
    "vec_to_array_udf = udf(vector_to_array, ArrayType(DoubleType()))\n",
    "df = df.withColumn(\"category_array\", vec_to_array_udf(col(\"category_vec\")))\n",
    "df = df.withColumn(\"gender_array\", vec_to_array_udf(col(\"gender_vec\")))\n",
    "\n",
    "# STEP 4: Get label names\n",
    "category_labels = model.stages[0].labels  # from category StringIndexer\n",
    "gender_labels = model.stages[1].labels    # from gender StringIndexer\n",
    "\n",
    "# Drop first to simulate drop_first=True like pandas\n",
    "category_dummies = [f\"category_{label}\" for label in category_labels[1:]]\n",
    "gender_dummies = [f\"gender_{label}\" for label in gender_labels[1:]]\n",
    "\n",
    "# STEP 5: Create individual columns for category\n",
    "for i, name in enumerate(category_dummies):\n",
    "    df = df.withColumn(name, col(\"category_array\")[i])\n",
    "\n",
    "# STEP 6: Create individual column for gender\n",
    "for i, name in enumerate(gender_dummies):\n",
    "    df = df.withColumn(name, col(\"gender_array\")[i])\n",
    "\n",
    "# STEP 7: Drop old and intermediate columns\n",
    "df = df.drop(\"category\", \"category_index\", \"category_vec\", \"category_array\")\n",
    "df = df.drop(\"gender\", \"gender_index\", \"gender_vec\", \"gender_array\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "283afcc7-0747-4380-ba4d-7c2f779caf87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, col\n",
    "\n",
    "# Step 1: Compute frequency of each city\n",
    "city_freq_df = df.groupBy(\"city\").agg(count(\"*\").alias(\"city_freq\"))\n",
    "\n",
    "# Step 2: Join back to the original dataframe\n",
    "df = df.join(city_freq_df, on=\"city\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a7e3dd7c-e3e0-4e3a-afd7-4957b4fa4c4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['city',\n",
       " 'Unnamed: 0',\n",
       " 'trans_date_trans_time',\n",
       " 'cc_num',\n",
       " 'merchant',\n",
       " 'amt',\n",
       " 'first',\n",
       " 'last',\n",
       " 'street',\n",
       " 'state',\n",
       " 'zip',\n",
       " 'lat',\n",
       " 'long',\n",
       " 'city_pop',\n",
       " 'job',\n",
       " 'trans_num',\n",
       " 'unix_time',\n",
       " 'merch_lat',\n",
       " 'merch_long',\n",
       " 'is_fraud',\n",
       " 'age',\n",
       " 'distance',\n",
       " 'amt_per_capita',\n",
       " 'is_far',\n",
       " 'trans_hour',\n",
       " 'trans_day',\n",
       " 'trans_weekday',\n",
       " 'hour_bucket_encoded',\n",
       " 'category_grocery_pos',\n",
       " 'category_home',\n",
       " 'category_shopping_pos',\n",
       " 'category_kids_pets',\n",
       " 'category_shopping_net',\n",
       " 'category_entertainment',\n",
       " 'category_food_dining',\n",
       " 'category_personal_care',\n",
       " 'category_health_fitness',\n",
       " 'category_misc_pos',\n",
       " 'category_misc_net',\n",
       " 'category_grocery_net',\n",
       " 'category_travel',\n",
       " 'gender_M',\n",
       " 'city_freq']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a4787f76-6f18-49af-864e-fb0853a542ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1296675, 43)\n"
     ]
    }
   ],
   "source": [
    "rows = df.count()\n",
    "cols = len(df.columns)\n",
    "print(f\"Shape: ({rows}, {cols})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab57aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['Unnamed: 0','city','trans_date_trans_time','cc_num','merchant','first',\n",
    " 'last','street','state','zip','lat','long','city_pop','job','trans_num','unix_time',\n",
    " 'merch_lat','merch_long','trans_hour','trans_day','trans_weekday']\n",
    "df = df.drop(*cols)\n",
    "# dropped 21 columns.            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "35b92994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amt',\n",
       " 'is_fraud',\n",
       " 'age',\n",
       " 'distance',\n",
       " 'amt_per_capita',\n",
       " 'is_far',\n",
       " 'hour_bucket_encoded',\n",
       " 'category_grocery_pos',\n",
       " 'category_home',\n",
       " 'category_shopping_pos',\n",
       " 'category_kids_pets',\n",
       " 'category_shopping_net',\n",
       " 'category_entertainment',\n",
       " 'category_food_dining',\n",
       " 'category_personal_care',\n",
       " 'category_health_fitness',\n",
       " 'category_misc_pos',\n",
       " 'category_misc_net',\n",
       " 'category_grocery_net',\n",
       " 'category_travel',\n",
       " 'gender_M',\n",
       " 'city_freq']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "52e80382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (1296675, 22)\n"
     ]
    }
   ],
   "source": [
    "rows = df.count()\n",
    "cols = len(df.columns)\n",
    "print(f\"Shape: ({rows}, {cols})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "668bb96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.coalesce(1).write.csv(\"processed_fraud_data_single3.csv\", header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44ec5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
